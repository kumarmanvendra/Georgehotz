{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, multiprocessing, numpy as np, torch, re, sys, simpleaudio as sa\n",
    "import whisper, llama, vits\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.helpers import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "args = AttrDict(**{\n",
    "    \"personality\": \"Stacy\",\n",
    "    \"speaker_id\": 6,\n",
    "    \"whisper_size\": \"tiny\",\n",
    "    \"llama_size\": \"7B\",\n",
    "    \"vits_model\": \"vctk\",\n",
    "    \"noise_scale\": 0.667,\n",
    "    \"noise_scale_w\": 0.8,\n",
    "    \"length_scale\": 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used: 14.81 GB, decoder.ln.bias                                   : 100%|██████████| 479/479 [00:01<00:00, 401.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 1195.80 ms, 14.81 GB loaded at 12.39 GB/s\n",
      "using 7B model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used: 27.90 GB, freqs_cis                                         : 100%|██████████| 292/292 [00:05<00:00, 49.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 5910.01 ms, 27.90 GB loaded at 4.72 GB/s\n",
      "Preparing KV cache for chatbot with personality Stacy...\n",
      "15958.52 ms\n"
     ]
    }
   ],
   "source": [
    "whisper_model, whisper_enc = whisper.load_model_and_enc(args.whisper_size == \"small\")\n",
    "\n",
    "llama_sp_model = llama.sp_model()\n",
    "llama_model = llama.load_model(args.llama_size)\n",
    "toks, start_pos, user_delim, end_delim = llama.encode_chatbot_preprompt(llama_model, llama_sp_model, args.personality)\n",
    "outputted = llama_sp_model.decode(toks) \n",
    "\n",
    "# VITS SETUP START. TODO: pretty much none of this code should exist in this file\n",
    "vits_model_config = vits.MODELS[args.vits_model]\n",
    "# Load the hyperparameters from the config file.\n",
    "config_path = vits_model_config[0]\n",
    "vits.download_if_not_present(config_path, vits_model_config[2])\n",
    "hps = vits.get_hparams_from_file(config_path)\n",
    "# Load symbols, instantiate TextMapper and clean the text.\n",
    "if hps.__contains__(\"symbols\"): symbols = hps.symbols\n",
    "elif args.vits_model == \"mmts-tts\": symbols = [x.replace(\"\\n\", \"\") for x in open(vits.download_if_not_present(vits.VITS_PATH / \"vocab_mmts-tts.txt\", \"https://huggingface.co/facebook/mms-tts/raw/main/full_models/eng/vocab.txt\"), encoding=\"utf-8\").readlines()]\n",
    "else: symbols = ['_'] + list(';:,.!?¡¿—…\"«»“” ') + list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz') + list(\"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\")\n",
    "vits_text_mapper = vits.TextMapper(apply_cleaners=True, symbols=symbols)\n",
    "vits_net_g = vits.load_model(vits_text_mapper.symbols, hps, vits_model_config)\n",
    "# VITS SETUP END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listening\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|notimestamps|>\n",
      "<|startoftranscript|><|notimestamps|> Hello\n",
      "<|startoftranscript|><|notimestamps|> Hello,\n",
      "<|startoftranscript|><|notimestamps|> Hello, how\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you doing\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you doing?\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you doing? Hello\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you doing? Hello,\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you doing? Hello, how\n",
      "done listening\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you doing? Hello, how are\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you doing? Hello, how are you\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you doing? Hello, how are you doing\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you doing? Hello, how are you doing?\n",
      "<|startoftranscript|><|notimestamps|> Hello, how are you doing? Hello, how are you doing?<|endoftext|>\n",
      " Hello, how are you doing? Hello, how are you doing?\n",
      "Stacy: I'm doing alright. I'm doing alright. [EOS]\n",
      "listening\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|notimestamps|>\n",
      "<|startoftranscript|><|notimestamps|> Well\n",
      "<|startoftranscript|><|notimestamps|> Well,\n",
      "<|startoftranscript|><|notimestamps|> Well, that\n",
      "<|startoftranscript|><|notimestamps|> Well, that's\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear.\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I'm\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I'm glad\n",
      "done listening\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I'm glad that\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I'm glad that you\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I'm glad that you're\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I'm glad that you're doing\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I'm glad that you're doing all\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I'm glad that you're doing all right\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I'm glad that you're doing all right.\n",
      "<|startoftranscript|><|notimestamps|> Well, that's good to hear. I'm glad that you're doing all right.<|endoftext|>\n",
      " Well, that's good to hear. I'm glad that you're doing all right.\n",
      "Stacy: Yes, thank you.  [EOS]\n",
      "listening\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|notimestamps|>\n",
      "<|startoftranscript|><|notimestamps|> How\n",
      "<|startoftranscript|><|notimestamps|> How many\n",
      "<|startoftranscript|><|notimestamps|> How many planets\n",
      "<|startoftranscript|><|notimestamps|> How many planets are\n",
      "<|startoftranscript|><|notimestamps|> How many planets are around\n",
      "<|startoftranscript|><|notimestamps|> How many planets are around the\n",
      "<|startoftranscript|><|notimestamps|> How many planets are around the Sun\n",
      "<|startoftranscript|><|notimestamps|> How many planets are around the Sun?\n",
      "<|startoftranscript|><|notimestamps|> How many planets are around the Sun?<|endoftext|>\n",
      " How many planets are around the Sun?\n",
      "Stacy: I'm not suredone listening\n",
      ", but there's 10 planets up there, and I've been rapping for over 10 years. [EOS]\n",
      "listening\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|notimestamps|>\n",
      "<|startoftranscript|><|notimestamps|> What\n",
      "<|startoftranscript|><|notimestamps|> What's\n",
      "<|startoftranscript|><|notimestamps|> What's your\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap lyric\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap lyric?\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap lyric? What\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap lyric? What's\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap lyric? What's your\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap lyric? What's your favorite\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap lyric? What's your favorite rap\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap lyric? What's your favorite rap lyric\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap lyric? What's your favorite rap lyric?\n",
      "<|startoftranscript|><|notimestamps|> What's your favorite rap lyric? What's your favorite rap lyric?<|endoftext|>\n",
      " What's your favorite rap lyric? What's your favorite rap lyric?\n",
      "done listening\n",
      "Stacy: My favorite rap lyric is, \"Shoot that, I'm like a glock, what you gon' do, you will back up, I don't, so you'll listen.\" [EOS]\n",
      "listening\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "done listening\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|notimestamps|>\n",
      "<|startoftranscript|><|notimestamps|> Could\n",
      "<|startoftranscript|><|notimestamps|> Could you\n",
      "<|startoftranscript|><|notimestamps|> Could you say\n",
      "<|startoftranscript|><|notimestamps|> Could you say that\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again?\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say that\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say that again\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say that again?\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say that again? Could\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say that again? Could you\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say that again? Could you say\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say that again? Could you say that\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say that again? Could you say that again\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say that again? Could you say that again?\n",
      "<|startoftranscript|><|notimestamps|> Could you say that again? Could you say that again? Could you say that again?<|endoftext|>\n",
      " Could you say that again? Could you say that again? Could you say that again?\n",
      "Stacy: I'm only saying that, because you asked me to say it again. [EOS]\n",
      "listening\n",
      "<|startoftranscript|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|nospeech|>\n",
      "<|startoftranscript|><|nospeech|><|endoftext|>\n",
      "<|startoftranscript|><|notimestamps|>\n",
      "<|startoftranscript|><|notimestamps|> Yeah\n",
      "<|startoftranscript|><|notimestamps|> Yeah,\n",
      "<|startoftranscript|><|notimestamps|> Yeah, I\n",
      "<|startoftranscript|><|notimestamps|> Yeah, I mean\n",
      "<|startoftranscript|><|notimestamps|> Yeah, I mean that\n",
      "<|startoftranscript|><|notimestamps|> Yeah, I mean that's\n",
      "<|startoftranscript|><|notimestamps|> Yeah, I mean that's fine\n",
      "<|startoftranscript|><|notimestamps|> Yeah, I mean that's fine.\n",
      "<|startoftranscript|><|notimestamps|> Yeah, I mean that's fine.<|endoftext|>\n",
      " Yeah, I mean that's fine.\n",
      "Stacy: That'done listening\n",
      "s fine. [EOS]\n",
      "listening\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[39m# whisper -> llama -> vits\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     user_speech \u001b[39m=\u001b[39m listen(whisper_model, whisper_enc)\n\u001b[1;32m     42\u001b[0m     tg_speech \u001b[39m=\u001b[39m process(user_speech)\n\u001b[1;32m     43\u001b[0m     speak(tg_speech)\n",
      "Cell \u001b[0;32mIn[93], line 18\u001b[0m, in \u001b[0;36mlisten\u001b[0;34m(model, enc)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m did_read:\n\u001b[1;32m     17\u001b[0m   log_spec \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39mprep_audio(torch\u001b[39m.\u001b[39mTensor(total), whisper\u001b[39m.\u001b[39mRATE)\n\u001b[0;32m---> 18\u001b[0m   encoded_audio \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencoder(Tensor(log_spec))\u001b[39m.\u001b[39;49mrealize()\n\u001b[1;32m     19\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecoder(Tensor([lst]), encoded_audio)\u001b[39m.\u001b[39mrealize()\n\u001b[1;32m     20\u001b[0m idx \u001b[39m=\u001b[39m out[\u001b[39m0\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39margmax()\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/tensor.py:94\u001b[0m, in \u001b[0;36mTensor.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrealize\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 94\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlazydata\u001b[39m.\u001b[39;49mrealize()\n\u001b[1;32m     95\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/lazy.py:147\u001b[0m, in \u001b[0;36mLazyBuffer.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m# run the ast if we still have to, and log the op\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized:\n\u001b[0;32m--> 147\u001b[0m   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mbuffers: x\u001b[39m.\u001b[39;49mrealize()\n\u001b[1;32m    149\u001b[0m   \u001b[39m# HACK: image shape can be wrong, hot cast it back to a normal float\u001b[39;00m\n\u001b[1;32m    150\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mis\u001b[39;00m ImageDType \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptype \u001b[39m!=\u001b[39m MovementOps \u001b[39mand\u001b[39;00m (prod(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m prod(cast(ImageDType, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mshape) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[x]\u001b[39m%\u001b[39m\u001b[39m4\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mst\u001b[39m.\u001b[39munit_stride_axes())):\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/lazy.py:147\u001b[0m, in \u001b[0;36mLazyBuffer.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m# run the ast if we still have to, and log the op\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized:\n\u001b[0;32m--> 147\u001b[0m   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mbuffers: x\u001b[39m.\u001b[39;49mrealize()\n\u001b[1;32m    149\u001b[0m   \u001b[39m# HACK: image shape can be wrong, hot cast it back to a normal float\u001b[39;00m\n\u001b[1;32m    150\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mis\u001b[39;00m ImageDType \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptype \u001b[39m!=\u001b[39m MovementOps \u001b[39mand\u001b[39;00m (prod(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m prod(cast(ImageDType, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mshape) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[x]\u001b[39m%\u001b[39m\u001b[39m4\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mst\u001b[39m.\u001b[39munit_stride_axes())):\n",
      "    \u001b[0;31m[... skipping similar frames: LazyBuffer.realize at line 147 (15 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/lazy.py:147\u001b[0m, in \u001b[0;36mLazyBuffer.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m# run the ast if we still have to, and log the op\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized:\n\u001b[0;32m--> 147\u001b[0m   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mbuffers: x\u001b[39m.\u001b[39;49mrealize()\n\u001b[1;32m    149\u001b[0m   \u001b[39m# HACK: image shape can be wrong, hot cast it back to a normal float\u001b[39;00m\n\u001b[1;32m    150\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mis\u001b[39;00m ImageDType \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptype \u001b[39m!=\u001b[39m MovementOps \u001b[39mand\u001b[39;00m (prod(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m prod(cast(ImageDType, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mshape) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[x]\u001b[39m%\u001b[39m\u001b[39m4\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mst\u001b[39m.\u001b[39munit_stride_axes())):\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/lazy.py:157\u001b[0m, in \u001b[0;36mLazyBuffer.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop \u001b[39m=\u001b[39m LazyOp(UnaryOps\u001b[39m.\u001b[39mCAST, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop,), dtypes\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    156\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mfloat32\n\u001b[0;32m--> 157\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized \u001b[39m=\u001b[39m Device[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice]\u001b[39m.\u001b[39;49mexec_ast(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mop, output\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device_extra_args())\n\u001b[1;32m    159\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized, (RawConst, Device[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice]\u001b[39m.\u001b[39mbuffer)), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdevice mismatch on realized got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized)\u001b[39m}\u001b[39;00m\u001b[39m expected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[39m# HACK: allow hot casting of images\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/ops.py:197\u001b[0m, in \u001b[0;36mCompiled.exec_ast\u001b[0;34m(self, ast, output, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m   prg \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mcodegen()\u001b[39m.\u001b[39mbuild(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mruntime)\n\u001b[1;32m    195\u001b[0m \u001b[39mif\u001b[39;00m prg\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m getenv(\u001b[39m\"\u001b[39m\u001b[39mPRINT_PRG\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mprint\u001b[39m(prg\u001b[39m.\u001b[39mprg)\n\u001b[0;32m--> 197\u001b[0m prg\u001b[39m.\u001b[39;49mexec(k\u001b[39m.\u001b[39;49mbufs)\n\u001b[1;32m    198\u001b[0m \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39mrealized\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/ops.py:139\u001b[0m, in \u001b[0;36mASTRunner.exec\u001b[0;34m(self, bufs, force_wait)\u001b[0m\n\u001b[1;32m    137\u001b[0m rawbufs \u001b[39m=\u001b[39m dedup([x\u001b[39m.\u001b[39mrealized \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m bufs \u001b[39mif\u001b[39;00m buf_is_kernel_arg(x)])\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m GlobalCounters\u001b[39m.\u001b[39mcache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: GlobalCounters\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39mappend((\u001b[39mself\u001b[39m, rawbufs))\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(rawbufs, force_wait\u001b[39m=\u001b[39;49mforce_wait)\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/ops.py:142\u001b[0m, in \u001b[0;36mASTRunner.__call__\u001b[0;34m(self, rawbufs, jit, force_wait)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, rawbufs:List[RawBuffer], jit\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, force_wait\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39mfloat\u001b[39m]:\n\u001b[0;32m--> 142\u001b[0m   \u001b[39mif\u001b[39;00m et \u001b[39m:=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclprg((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobal_size \u001b[39m+\u001b[39;49m [\u001b[39m1\u001b[39;49m]\u001b[39m*\u001b[39;49m(\u001b[39m3\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobal_size))) \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobal_size \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    143\u001b[0m                       (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_size \u001b[39m+\u001b[39;49m [\u001b[39m1\u001b[39;49m]\u001b[39m*\u001b[39;49m(\u001b[39m3\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_size))) \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_size \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    144\u001b[0m                       \u001b[39m*\u001b[39;49mrawbufs, wait\u001b[39m=\u001b[39;49mforce_wait \u001b[39mor\u001b[39;49;00m DEBUG\u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m): GlobalCounters\u001b[39m.\u001b[39mtime_sum_s \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m et\n\u001b[1;32m    145\u001b[0m   \u001b[39mif\u001b[39;00m DEBUG \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/runtime/ops_metal.py:67\u001b[0m, in \u001b[0;36mMetalProgram.__call__\u001b[0;34m(self, global_size, local_size, wait, *bufs)\u001b[0m\n\u001b[1;32m     65\u001b[0m encoder\u001b[39m.\u001b[39msetComputePipelineState_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline_state)\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m i,a \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(bufs): encoder\u001b[39m.\u001b[39msetBuffer_offset_atIndex_(a\u001b[39m.\u001b[39m_buf, \u001b[39m0\u001b[39m, i)\n\u001b[0;32m---> 67\u001b[0m encoder\u001b[39m.\u001b[39;49mdispatchThreadgroups_threadsPerThreadgroup_(Metal\u001b[39m.\u001b[39;49mMTLSize(\u001b[39m*\u001b[39;49mglobal_size), Metal\u001b[39m.\u001b[39;49mMTLSize(\u001b[39m*\u001b[39;49mlocal_size))\n\u001b[1;32m     68\u001b[0m encoder\u001b[39m.\u001b[39mendEncoding()\n\u001b[1;32m     69\u001b[0m command_buffer\u001b[39m.\u001b[39mcommit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done listening\n"
     ]
    }
   ],
   "source": [
    "def listen(model, enc) -> str:\n",
    "  # TODO: i suspect https://github.com/tinygrad/tinygrad/pull/999/files will make this better\n",
    "  q = multiprocessing.Queue()\n",
    "  p = multiprocessing.Process(target=whisper.listener, args=(q,))\n",
    "  p.daemon = True\n",
    "  p.start()\n",
    "  lst = [whisper_enc._special_tokens[\"<|startoftranscript|>\"]]\n",
    "  total = None\n",
    "  did_read = False\n",
    "  for i in range(0, int(whisper.RATE / whisper.CHUNK * whisper.RECORD_SECONDS)):\n",
    "    while not q.empty() or total is None:\n",
    "      waveform = q.get()\n",
    "      if total is None: total = waveform\n",
    "      else: total = np.concatenate([total, waveform], axis=1)\n",
    "      did_read = True\n",
    "    if did_read:\n",
    "      log_spec = whisper.prep_audio(torch.Tensor(total), whisper.RATE)\n",
    "      encoded_audio = model.encoder(Tensor(log_spec)).realize()\n",
    "    out = model.decoder(Tensor([lst]), encoded_audio).realize()\n",
    "    idx = out[0,-1].numpy().argmax()\n",
    "    lst.append(idx)\n",
    "    dec = whisper_enc.decode(lst)\n",
    "    if dec.endswith(\"<|endoftext|>\"):\n",
    "      total = total[:, 320*(len(lst)-1):]\n",
    "      lst = [whisper_enc._special_tokens[\"<|startoftranscript|>\"]]\n",
    "      if len(user_speech := re.sub('<\\|.*?\\|>', '', dec)) > 0:\n",
    "        break\n",
    "  print(user_speech)\n",
    "  return user_speech\n",
    "\n",
    "\n",
    "def process(input: str) -> str:\n",
    "  pass\n",
    "\n",
    "def speak(input: str):\n",
    "  pass\n",
    "\n",
    "while 1:\n",
    "    # whisper -> llama -> vits\n",
    "    user_speech = listen(whisper_model, whisper_enc)\n",
    "    tg_speech = process(user_speech)\n",
    "    speak(tg_speech)\n",
    "\n",
    "    # add tokens from user in chatbot mode\n",
    "    outputted += user_delim + user_speech + \"\\n\"\n",
    "\n",
    "    new_toks = [llama_sp_model.bos_id()] + llama_sp_model.encode(outputted)\n",
    "    assert toks == new_toks[:len(toks)]\n",
    "    toks = new_toks\n",
    "    assert outputted == llama_sp_model.decode(toks)\n",
    "\n",
    "    text_to_synthesize = \"\"\n",
    "\n",
    "    while 1:\n",
    "      logits = llama_model(Tensor([toks[start_pos:]]), start_pos).realize()\n",
    "      tok = llama.sample(logits, 0.7) # args.temperature in llama.py\n",
    "      # use the kv cache\n",
    "      start_pos = len(toks)\n",
    "      # add the new token\n",
    "      toks.append(tok)\n",
    "      # TODO: this is a hack to deal with spaces. i think the decode is fast though, so who cares?\n",
    "      cur = llama_sp_model.decode(toks)\n",
    "      out = cur[len(outputted):]\n",
    "      text_to_synthesize += out\n",
    "      sys.stdout.write(out)\n",
    "      sys.stdout.flush()\n",
    "      outputted = cur\n",
    "      if outputted.endswith(end_delim): break\n",
    "\n",
    "    text_to_synthesize = text_to_synthesize.split(\": \", 1)[1].replace(\" [EOS]\", \"\")\n",
    "\n",
    "    # text-to-speech\n",
    "    if args.vits_model == \"mmts-tts\": text_to_synthesize = vits_text_mapper.filter_oov(text_to_synthesize.lower())\n",
    "    stn_tst = vits_text_mapper.get_text(text_to_synthesize, hps.data.add_blank, hps.data.text_cleaners)\n",
    "    x_tst, x_tst_lengths = stn_tst.unsqueeze(0), Tensor([stn_tst.shape[0]], dtype=dtypes.int64)\n",
    "    sid = Tensor([args.speaker_id], dtype=dtypes.int64)\n",
    "    audio_tensor = vits_net_g.infer(x_tst, x_tst_lengths, sid, args.noise_scale, args.length_scale, args.noise_scale_w, emotion_embedding=None,max_y_length_estimate_scale=None)[0, 0].realize()\n",
    "    audio_data = (np.clip(audio_tensor.numpy(), -1.0, 1.0) * 32767).astype(np.int16)\n",
    "    sa.play_buffer(audio_data, 1, 2, hps.data.sampling_rate).wait_done()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
