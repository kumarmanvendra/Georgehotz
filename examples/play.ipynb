{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, multiprocessing as mp, numpy as np, torch, re, sys, simpleaudio as sa\n",
    "import whisper, llama, vits\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.helpers import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "args = AttrDict(**{\n",
    "    \"personality\": \"Stacy\",\n",
    "    \"speaker_id\": 6,\n",
    "    \"whisper_size\": \"tiny\",\n",
    "    \"llama_size\": \"7B\",\n",
    "    \"vits_model\": \"vctk\",\n",
    "    \"noise_scale\": 0.667,\n",
    "    \"noise_scale_w\": 0.8,\n",
    "    \"length_scale\": 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done listening\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used: 14.13 GB, encoder.conv2.weight                              :   0%|          | 0/167 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used: 14.24 GB, decoder.ln.bias                                   : 100%|██████████| 167/167 [00:00<00:00, 1252.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 134.62 ms, 14.24 GB loaded at 105.79 GB/s\n",
      "using 7B model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used: 27.67 GB, freqs_cis                                         : 100%|██████████| 292/292 [00:08<00:00, 35.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 8301.13 ms, 27.67 GB loaded at 3.33 GB/s\n",
      "Preparing KV cache for chatbot with personality Stacy...\n",
      "16115.22 ms\n"
     ]
    }
   ],
   "source": [
    "whisper_model, whisper_enc = whisper.load_model_and_enc(args.whisper_size == \"small\")\n",
    "\n",
    "llama_sp_model = llama.sp_model()\n",
    "llama_model = llama.load_model(args.llama_size)\n",
    "toks, start_pos, user_delim, end_delim = llama.encode_chatbot_preprompt(llama_model, llama_sp_model, args.personality)\n",
    "outputted = llama_sp_model.decode(toks) \n",
    "\n",
    "# VITS SETUP START. TODO: pretty much none of this code should exist in this file\n",
    "vits_model_config = vits.MODELS[args.vits_model]\n",
    "# Load the hyperparameters from the config file.\n",
    "config_path = vits_model_config[0]\n",
    "vits.download_if_not_present(config_path, vits_model_config[2])\n",
    "hps = vits.get_hparams_from_file(config_path)\n",
    "# Load symbols, instantiate TextMapper and clean the text.\n",
    "if hps.__contains__(\"symbols\"): symbols = hps.symbols\n",
    "elif args.vits_model == \"mmts-tts\": symbols = [x.replace(\"\\n\", \"\") for x in open(vits.download_if_not_present(vits.VITS_PATH / \"vocab_mmts-tts.txt\", \"https://huggingface.co/facebook/mms-tts/raw/main/full_models/eng/vocab.txt\"), encoding=\"utf-8\").readlines()]\n",
    "else: symbols = ['_'] + list(';:,.!?¡¿—…\"«»“” ') + list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz') + list(\"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\")\n",
    "vits_text_mapper = vits.TextMapper(apply_cleaners=True, symbols=symbols)\n",
    "vits_net_g = vits.load_model(vits_text_mapper.symbols, hps, vits_model_config)\n",
    "# VITS SETUP END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listening\n",
      "You:  I have retraced my fur at the haps on the step and start.\n",
      "Stacy:  I have re-traineddone listening\n",
      " my fur at the haps on the step and start. [EOS]\n",
      "listening\n",
      "You:  China's a China's a\n",
      "Stacy:  China's a China's a China's a China's a China's a China's a China'sdone listening\n",
      " a China's a China's a China's a China's a China's a China's a China's a China'"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m# llama loop\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m   logits \u001b[39m=\u001b[39m llama_model(Tensor([toks[start_pos:]]), start_pos)\u001b[39m.\u001b[39mrealize()\n\u001b[1;32m     67\u001b[0m   tok \u001b[39m=\u001b[39m llama\u001b[39m.\u001b[39msample(logits, \u001b[39m0.7\u001b[39m) \u001b[39m# args.temperature in llama.py\u001b[39;00m\n\u001b[1;32m     68\u001b[0m   \u001b[39m# use the kv cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tinygrad/examples/llama.py:147\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, tokens, start_pos)\u001b[0m\n\u001b[1;32m    145\u001b[0m freqs_cis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfreqs_cis[:, start_pos:start_pos\u001b[39m+\u001b[39mseqlen]\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mrealize()\n\u001b[1;32m    146\u001b[0m mask \u001b[39m=\u001b[39m Tensor\u001b[39m.\u001b[39mfull((\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, seqlen, start_pos \u001b[39m+\u001b[39m seqlen), \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m\"\u001b[39m), dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mtriu(start_pos\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrealize() \u001b[39mif\u001b[39;00m seqlen \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m h \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39;49msequential([functools\u001b[39m.\u001b[39;49mpartial(layer, start_pos\u001b[39m=\u001b[39;49mstart_pos, freqs_cis\u001b[39m=\u001b[39;49mfreqs_cis, mask\u001b[39m=\u001b[39;49mmask) \u001b[39mfor\u001b[39;49;00m layer \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers])\n\u001b[1;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(h)[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/tensor.py:645\u001b[0m, in \u001b[0;36mTensor.sequential\u001b[0;34m(self, ll)\u001b[0m\n\u001b[0;32m--> 645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msequential\u001b[39m(\u001b[39mself\u001b[39m, ll:List[Callable[[Tensor], Tensor]]): \u001b[39mreturn\u001b[39;00m reduce(\u001b[39mlambda\u001b[39;49;00m x,f: f(x), ll, \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/tensor.py:645\u001b[0m, in \u001b[0;36mTensor.sequential.<locals>.<lambda>\u001b[0;34m(x, f)\u001b[0m\n\u001b[0;32m--> 645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msequential\u001b[39m(\u001b[39mself\u001b[39m, ll:List[Callable[[Tensor], Tensor]]): \u001b[39mreturn\u001b[39;00m reduce(\u001b[39mlambda\u001b[39;00m x,f: f(x), ll, \u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/tinygrad/examples/llama.py:130\u001b[0m, in \u001b[0;36mTransformerBlock.__call__\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m# inner_attention can't be jitted because it's dynamic based on start_pos\u001b[39;00m\n\u001b[1;32m    129\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention\u001b[39m.\u001b[39minner_attention(xq, xk, xv, start_pos, mask)\n\u001b[0;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(x, output)\n",
      "File \u001b[0;32m~/Desktop/tinygrad/examples/llama.py:124\u001b[0m, in \u001b[0;36mTransformerBlock.post\u001b[0;34m(self, x, output)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\u001b[39mself\u001b[39m, x:Tensor, output:Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    123\u001b[0m   h \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention\u001b[39m.\u001b[39mwo(output)\n\u001b[0;32m--> 124\u001b[0m   \u001b[39mreturn\u001b[39;00m (h \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mffn_norm(h)))\u001b[39m.\u001b[39;49mrealize()\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/tensor.py:94\u001b[0m, in \u001b[0;36mTensor.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrealize\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 94\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlazydata\u001b[39m.\u001b[39;49mrealize()\n\u001b[1;32m     95\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/lazy.py:147\u001b[0m, in \u001b[0;36mLazyBuffer.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m# run the ast if we still have to, and log the op\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized:\n\u001b[0;32m--> 147\u001b[0m   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mbuffers: x\u001b[39m.\u001b[39;49mrealize()\n\u001b[1;32m    149\u001b[0m   \u001b[39m# HACK: image shape can be wrong, hot cast it back to a normal float\u001b[39;00m\n\u001b[1;32m    150\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mis\u001b[39;00m ImageDType \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptype \u001b[39m!=\u001b[39m MovementOps \u001b[39mand\u001b[39;00m (prod(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m prod(cast(ImageDType, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mshape) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[x]\u001b[39m%\u001b[39m\u001b[39m4\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mst\u001b[39m.\u001b[39munit_stride_axes())):\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/lazy.py:147\u001b[0m, in \u001b[0;36mLazyBuffer.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m# run the ast if we still have to, and log the op\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized:\n\u001b[0;32m--> 147\u001b[0m   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mbuffers: x\u001b[39m.\u001b[39;49mrealize()\n\u001b[1;32m    149\u001b[0m   \u001b[39m# HACK: image shape can be wrong, hot cast it back to a normal float\u001b[39;00m\n\u001b[1;32m    150\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mis\u001b[39;00m ImageDType \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptype \u001b[39m!=\u001b[39m MovementOps \u001b[39mand\u001b[39;00m (prod(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m prod(cast(ImageDType, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mshape) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[x]\u001b[39m%\u001b[39m\u001b[39m4\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mst\u001b[39m.\u001b[39munit_stride_axes())):\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/lazy.py:147\u001b[0m, in \u001b[0;36mLazyBuffer.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m# run the ast if we still have to, and log the op\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized:\n\u001b[0;32m--> 147\u001b[0m   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mbuffers: x\u001b[39m.\u001b[39;49mrealize()\n\u001b[1;32m    149\u001b[0m   \u001b[39m# HACK: image shape can be wrong, hot cast it back to a normal float\u001b[39;00m\n\u001b[1;32m    150\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mis\u001b[39;00m ImageDType \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptype \u001b[39m!=\u001b[39m MovementOps \u001b[39mand\u001b[39;00m (prod(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m prod(cast(ImageDType, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mshape) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[x]\u001b[39m%\u001b[39m\u001b[39m4\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mst\u001b[39m.\u001b[39munit_stride_axes())):\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/lazy.py:157\u001b[0m, in \u001b[0;36mLazyBuffer.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop \u001b[39m=\u001b[39m LazyOp(UnaryOps\u001b[39m.\u001b[39mCAST, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop,), dtypes\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    156\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mfloat32\n\u001b[0;32m--> 157\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized \u001b[39m=\u001b[39m Device[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice]\u001b[39m.\u001b[39;49mexec_ast(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mop, output\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device_extra_args())\n\u001b[1;32m    159\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized, (RawConst, Device[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice]\u001b[39m.\u001b[39mbuffer)), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdevice mismatch on realized got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized)\u001b[39m}\u001b[39;00m\u001b[39m expected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[39m# HACK: allow hot casting of images\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/ops.py:190\u001b[0m, in \u001b[0;36mCompiled.exec_ast\u001b[0;34m(self, ast, output, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m       hand_coded_optimizations(k)\n\u001b[1;32m    189\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod_cache[k\u001b[39m.\u001b[39mkey] \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mcodegen()\u001b[39m.\u001b[39mbuild(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mruntime)\n\u001b[0;32m--> 190\u001b[0m   \u001b[39melif\u001b[39;00m DEBUG \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m: \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmethod cache hit : \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m.\u001b[39mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    191\u001b[0m   prg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod_cache[k\u001b[39m.\u001b[39mkey]\n\u001b[1;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/tinygrad/tinygrad/helpers.py:45\u001b[0m, in \u001b[0;36mContextVar.__ge__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x): ContextVar\u001b[39m.\u001b[39mctx_stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey] \u001b[39m=\u001b[39m x\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__bool__\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__ge__\u001b[39m(\u001b[39mself\u001b[39m, x): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m x\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__gt__\u001b[39m(\u001b[39mself\u001b[39m, x): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue \u001b[39m>\u001b[39m x\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__lt__\u001b[39m(\u001b[39mself\u001b[39m, x): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue \u001b[39m<\u001b[39m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def speak(q_speak, q_aid):\n",
    "  while 1:\n",
    "    if not q_speak.is_empty():\n",
    "      id, text_to_synthesize = q_speak.get()\n",
    "      if args.vits_model == \"mmts-tts\": text_to_synthesize = vits_text_mapper.filter_oov(text_to_synthesize.lower())\n",
    "      stn_tst = vits_text_mapper.get_text(text_to_synthesize, hps.data.add_blank, hps.data.text_cleaners)\n",
    "      x_tst, x_tst_lengths = stn_tst.unsqueeze(0), Tensor([stn_tst.shape[0]], dtype=dtypes.int64)\n",
    "      sid = Tensor([args.speaker_id], dtype=dtypes.int64)\n",
    "      audio_tensor = vits_net_g.infer(x_tst, x_tst_lengths, sid, args.noise_scale, args.length_scale, args.noise_scale_w, emotion_embedding=None,max_y_length_estimate_scale=None)[0, 0].realize()\n",
    "      audio_data = (np.clip(audio_tensor.numpy(), -1.0, 1.0) * 32767).astype(np.int16)\n",
    "      sa.play_buffer(audio_data, 1, 2, hps.data.sampling_rate).wait_done() \n",
    "      q_aid.put(id)\n",
    "\n",
    "q_speak, q_aid = mp.Queue(), mp.Queue()\n",
    "speak_p = mp.Process(target=speak, args=(q_speak,q_aid))\n",
    "audio_id = 0\n",
    "\n",
    "# chat loop\n",
    "while 1:\n",
    "  # TODO: when https://github.com/tinygrad/tinygrad/pull/999 is merged, this will work better\n",
    "  q = mp.Queue()\n",
    "  p = mp.Process(target=whisper.listener, args=(q,))\n",
    "  p.daemon = True\n",
    "  p.start()\n",
    "\n",
    "  lst = [whisper_enc._special_tokens[\"<|startoftranscript|>\"]]\n",
    "  total = None\n",
    "  did_read = False\n",
    "  for i in range(0, int(whisper.RATE / whisper.CHUNK * whisper.RECORD_SECONDS)):\n",
    "    while not q.empty() or total is None:\n",
    "      waveform = q.get()\n",
    "      if total is None: total = waveform\n",
    "      else: total = np.concatenate([total, waveform], axis=1)\n",
    "      did_read = True\n",
    "    if did_read:\n",
    "      last_total = total.shape[1]\n",
    "      log_spec = whisper.prep_audio(torch.Tensor(total), whisper.RATE)\n",
    "      encoded_audio = whisper_model.encoder(Tensor(log_spec)).realize()\n",
    "    out = whisper_model.decoder(Tensor([lst]), encoded_audio).realize()\n",
    "    idx = out[0,-1].numpy().argmax()\n",
    "    lst.append(idx)\n",
    "    dec = whisper_enc.decode(lst)\n",
    "    user_speech = re.sub('<\\|.*?\\|>', '', dec)\n",
    "    print(f\"You: {user_speech}\", end='\\r')\n",
    "    if dec.endswith(\"<|endoftext|>\"):\n",
    "      #total = total[:, 320*(len(lst)-1):]\n",
    "      lst = [whisper_enc._special_tokens[\"<|startoftranscript|>\"]]\n",
    "      if len(user_speech) > 0:\n",
    "        break\n",
    "\n",
    "  print(f\"You: {user_speech}\")\n",
    "\n",
    "  # throw into llama\n",
    "  # add tokens from user in chatbot mode\n",
    "  outputted += user_delim + user_speech + \"\\n\"\n",
    "\n",
    "  new_toks = [llama_sp_model.bos_id()] + llama_sp_model.encode(outputted)\n",
    "  assert toks == new_toks[:len(toks)]\n",
    "  toks = new_toks\n",
    "  assert outputted == llama_sp_model.decode(toks)\n",
    "\n",
    "  text_to_synthesize = \"\"\n",
    "\n",
    "  # llama loop\n",
    "  while 1:\n",
    "    logits = llama_model(Tensor([toks[start_pos:]]), start_pos).realize()\n",
    "    tok = llama.sample(logits, 0.7) # args.temperature in llama.py\n",
    "    # use the kv cache\n",
    "    start_pos = len(toks)\n",
    "    # add the new token\n",
    "    toks.append(tok)\n",
    "    # TODO: this is a hack to deal with spaces. i think the decode is fast though, so who cares?\n",
    "    cur = llama_sp_model.decode(toks)\n",
    "    out = cur[len(outputted):]\n",
    "    text_to_synthesize += out\n",
    "    sys.stdout.write(out)\n",
    "    sys.stdout.flush()\n",
    "    outputted = cur\n",
    "    if outputted.endswith((\"!\",\".\",\"?\")):\n",
    "      text_to_synthesize = text_to_synthesize.split(\": \", 1)[-1].replace(\" [EOS]\", \"\")\n",
    "      q_speak.put((audio_id, text_to_synthesize))\n",
    "      audio_id += 1\n",
    "      text_to_synthesize = \"\"\n",
    "    if outputted.endswith(end_delim): break\n",
    "\n",
    "  # wait until \"speak\" process is complete\n",
    "  while not q_aid.empty() and q_aid.get() >= audio_id:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
